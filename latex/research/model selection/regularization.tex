\subsection{Techniques to avoid overfitting} 

Training multiple models, we have noticed that oftentimes the model overfits on the training data and doesn't perform that good on the validation set with real data. To improve the performance and avoid overfitting we have deployed various techniques, explained in Section \ref{sec:parametersNetwork}. The application of these techniques to the baseline model and its results are described here. 

\subsubsection{Dropout}

Dropout is a technique that randomly stops using some of the neurons from the network. We use this in both, convolutional and fully-connected layers. It is important to mention that the probability of the dropout depends on the layer in which we use it. For the convolution layer, it is common to use lower probability between 0.2 and 0.3, while in the fully-connected layer higher values between 0.4 and 0.5 are used.

We applied dropout with different values of the probability to our baseline model and the results on the validation set are shown in the Table \ref{tab:dropouts}. 

{\renewcommand{\arraystretch}{1.4}
\begin{table}[h]
\centering
\begin{tabular}{|l|cc|cc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Dropout}}} & \multicolumn{2}{c|}{\textbf{Validation loss}} & \multicolumn{2}{c|}{\textbf{Validation accuracy (\%)}} \\ \cline{2-5} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{real data} & synthetic data & \multicolumn{1}{c|}{real data} & synthetic data \\ \hline
\textit{FC (0.5)} & \multicolumn{1}{c|}{1.017293} & 0.006852 & \multicolumn{1}{c|}{79.33} & 99.31 \\ \hline
\textit{FC (0.4)} & \multicolumn{1}{c|}{1.863367} & 0.004227 & \multicolumn{1}{c|}{77.33} & 99.61 \\ \hline
\textit{CON (0.2)} & \multicolumn{1}{c|}{1.175975} & 0.007962 & \multicolumn{1}{c|}{74.33} & 99.34 \\ \hline
\textit{CONV (0.3)} & \multicolumn{1}{c|}{0.670561} & 0.011941 & \multicolumn{1}{c|}{75.99} & 98.98 \\ \hline
\textit{FC (0.5) CONV (0.2)} & \multicolumn{1}{c|}{1.430928} & 0.009755 & \multicolumn{1}{c|}{73.00} & 99.14 \\ \hline
\textit{FC (0.5) CONV (0.3)} & \multicolumn{1}{c|}{0.908083} & 0.062156 & \multicolumn{1}{c|}{70.66} & 93.07 \\ \hline
\textit{FC (0.4) CONV (0.2)} & \multicolumn{1}{c|}{0.838473} & 0.009400 & \multicolumn{1}{c|}{75.99} & 99.11 \\ \hline
\textit{FC (0.4) CONV (0.3)} & \multicolumn{1}{c|}{0.643167} & 0.017051 & \multicolumn{1}{c|}{72.33} & 98.38 \\ \hline
\end{tabular}
\caption{Validation loss and accuracy on multiple models using dropout with various probability values.}
\label{tab:dropouts}
\end{table}
}

The dropout in the FC layers has performed a bit better than in the convolutional layers. We thought that combining them would achieve even better results but it has decreased the performance slightly. 


\subsubsection{L2 regularization}

Another useful technique to prevent the network from overfitting is using L2 regularization, which is added to the loss function. It is a function of weights and does not rely on the input data as the loss function does. How much the L2 regularization component contributes to the loss function is defined by the parameter $\lambda$ and it usually ranges from 0 to 0.1. 

The L2 regularization was added to the baseline model, with various values of $\lambda$ parameter as can be seen in the Table \ref{tab:regularization}. 

{\renewcommand{\arraystretch}{1.4}
\begin{table}[h]
\centering
\begin{tabular}{|l|cc|cc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{L2 regularization}}} & \multicolumn{2}{c|}{\textbf{Validation loss}} & \multicolumn{2}{c|}{\textbf{Validation accuracy (\%)}} \\ \cline{2-5} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{real data} & synthetic data & \multicolumn{1}{c|}{real data} & synthetic data \\ \hline
\textit{$\lambda = 1e^{-5}$} & \multicolumn{1}{c|}{1.546157} & 0.005279 & \multicolumn{1}{c|}{75.33} & 99.51 \\ \hline
\textit{$\lambda = 1e^{-4}$} & \multicolumn{1}{c|}{0.952916} & 0.007885 & \multicolumn{1}{c|}{75.00} & 99.23 \\ \hline
\textit{$\lambda = 1e^{-3}$} & \multicolumn{1}{c|}{0.858371} & 0.039260 & \multicolumn{1}{c|}{63.33} & 96.82 \\ \hline
\textit{$\lambda = 1e^{-2}$} & \multicolumn{1}{c|}{0.451789} & 0.451778 & \multicolumn{1}{c|}{18.33} & 16.66 \\ \hline
\end{tabular}
\caption{Validation loss and accuracy on multiple models with different values of $\lambda$ of L2 regularization.}
\label{tab:regularization}
\end{table}
}

Based on the results in the Table \ref{tab:regularization} we can say that the lower the value of $\lambda$ better the performance of our model. Again, it's interesting to see that the model with the highest loss on the real data has also the highest accuracy. With the $\lambda$ set to $1e^{-2}$, the L2 regularization component overwhelmed the whole loss function. The loss for real and synthetic data is almost the same because the loss function contains only the L2 regularization component, which is the function of weights. This caused the network to not learn anything and the accuracy being extremely low. 

\subsubsection{Scheduler}

We mentioned how important is the optimizer in the training of the network, and how the learning rate influences its performance significantly. Most of the time it is not beneficial to use the same learning rate during the whole training process, which is why we deploy learning rate schedulers. 

In our work, we have selected two different types of schedulers. The $ReduceLROn\\Plateau$ scheduler reduces the learning rate when the validation loss stops improving for a certain period. The other deployed scheduler is $Exponential$, which reduces the learning rate by a factor of $\gamma$ after each epoch. 

The performance of the baseline model using the schedulers is illustrated in the Table \ref{tab:schedulerpar}. Based on these results we can say that the $Exponential$ scheduler with gamma set to 0.9 performed the best, but $ReduceLROnPlateau$ is not very far behind. However comparing this to the baseline model without a scheduler, it performed much better (accuracy 79 \%). 

{\renewcommand{\arraystretch}{1.4}
\begin{table}[h]
\begin{tabular}{|l|l|cc|cc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Scheduler}}} & \multirow{2}{*}{\textbf{Parameters}} & \multicolumn{2}{c|}{\textbf{Validation loss}} & \multicolumn{2}{c|}{\textbf{Validation accuracy (\%)}} \\ \cline{3-6} 
\multicolumn{1}{|c|}{} &  & \multicolumn{1}{c|}{real data} & \begin{tabular}[c]{@{}c@{}}synthetic \\ data\end{tabular} & \multicolumn{1}{c|}{real data} & \begin{tabular}[c]{@{}c@{}}synthetic \\ data\end{tabular} \\ \hline
\textit{\begin{tabular}[c]{@{}l@{}}ReduceLR\\ OnPlateau\end{tabular}} & \begin{tabular}[c]{@{}l@{}}factor = 0.1\\ thresh = $1e^{-4}$\end{tabular} & \multicolumn{1}{c|}{1.564949} & 0.010585 & \multicolumn{1}{c|}{72.00} & 99.02 \\ \hline
\textit{\begin{tabular}[c]{@{}l@{}}ReduceLR\\ OnPlateau\end{tabular}} & \begin{tabular}[c]{@{}l@{}}factor = 0.9\\ thresh = $1e^{-2}$\end{tabular} & \multicolumn{1}{c|}{1.990145} & 0.006156 & \multicolumn{1}{c|}{73.66} & 99.42 \\ \hline
\textit{Exponential} & gamma = 0.9 & \multicolumn{1}{c|}{2.958377} & 0.005198 & \multicolumn{1}{c|}{75.66} & 99.58 \\ \hline
\textit{Exponential} & gamma = 0.8 & \multicolumn{1}{c|}{2.915110} & 0.008034 & \multicolumn{1}{c|}{69.66} & 99.29 \\ \hline
\textit{Exponential} & gamma = 0.7 & \multicolumn{1}{c|}{2.353500} & 0.009336 & \multicolumn{1}{c|}{70.66} & 99.09 \\ \hline
\end{tabular}
\caption{Validation loss and accuracy on multiple models with different values of schdulers and their parameters.}
\label{tab:schedulerpar}
\end{table}
}
