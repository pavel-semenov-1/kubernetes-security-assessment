\subsection{Activation function}

The activation function introduces non-linearity into the neural networks. In our model, it is applied after each convolutional and fully-connected layer and it is the same function in all layers. The baseline model was trained using multiple activation functions with various parameters as shown in the Table \ref{tab:actfuncs}. It shows the validation loss and accuracy of the trained model in the early stopping checkpoint. 

{\renewcommand{\arraystretch}{1.4}
\begin{table}[h]
\centering
\begin{tabular}{|l|cc|cc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Activation function}}} & \multicolumn{2}{c|}{\textbf{Validation loss}} & \multicolumn{2}{c|}{\textbf{Validation accuracy (\%)}} \\ \cline{2-5} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{real data} & synthetic data & \multicolumn{1}{c|}{real data} & synthetic data \\ \hline
\textit{sigmoid} & \multicolumn{1}{c|}{1.084613} & 0.059421 & \multicolumn{1}{c|}{63.99} & 97.74 \\ \hline
\textit{tanh} & \multicolumn{1}{c|}{0.897948} & 0.033934 & \multicolumn{1}{c|}{73.66} & 98.48 \\ \hline
\textit{RELU} & \multicolumn{1}{c|}{2.377840} & 0.004796 & \multicolumn{1}{c|}{75.33} & 99.54 \\ \hline
\textit{Leaky RELU (0.05)} & \multicolumn{1}{c|}{1.017293} & 0.006852 & \multicolumn{1}{c|}{79.33} & 99.31 \\ \hline
\textit{Leaky RELU (0.1)} & \multicolumn{1}{c|}{1.221875} & 0.005321 & \multicolumn{1}{c|}{77.00} & 99.52 \\ \hline
\textit{Leaky RELU (0.4)} & \multicolumn{1}{c|}{0.443687} & 0.008263 & \multicolumn{1}{c|}{79.00} & 99.24 \\ \hline
\textit{Leaky RELU (0.5)} & \multicolumn{1}{c|}{0.417411} & 0.018245 & \multicolumn{1}{c|}{79.33} & 98.58 \\ \hline
\textit{Leaky RELU (0.6)} & \multicolumn{1}{c|}{0.450320} & 0.014370 & \multicolumn{1}{c|}{77.33} & 98.76 \\ \hline
\textit{Leaky RELU (0.7)} & \multicolumn{1}{c|}{0.445662} & 0.021970 & \multicolumn{1}{c|}{74.66} & 98.40 \\ \hline
\end{tabular}
\caption{Validation loss and accuracy on multiple models with different activation functions.}
\label{tab:actfuncs}
\end{table}}


We can see that most of the activation functions performed better than 70 \% aside from the sigmoid function. This corresponds with the practical experience of many people since sigmoid is not used in neural networks anymore due to its problems. On the other hand, one of the best performing functions in our model is Leaky RELU. Surprising is that the tanh activation performed almost as better as RELU even though it's not very popular in the state-of-the-art models. 
