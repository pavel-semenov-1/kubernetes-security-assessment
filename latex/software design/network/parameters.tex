\subsection{Parameters} \label{sec:parametersNetwork}

% softmax na konci
% cross entropy na pocitanie gradientu
% early stopping pre val loss aj accuracy
% parametre optimizeru
% scheduler pre LR
% regularization, dropout
% augumentation
% activation function


During the training, images fed to the network come in batches, whose size is determined by parameter \textit{batch size}. Each batch of images is passed through the layers of the network. After each convolution layer, \textit{activation function} is applied to the output. Before the image is passed into the fully-connected layer, it must be flattened into a one-dimensional vector. The last fully-connected layer outputs the class scores of the image. \textit{Softmax activation} is applied to class scores to normalize them into class probabilities, where each value represents how sure our network is that the input is the specific class. A useful feature of softmax activation is that adding probability values for each class together outputs 1. 

\textit{Loss function} is used to measure the performance of the network. It consists of two components: data loss and regularization loss. Data loss calculates the quality of the prediction compared to ground truth labels, for which we use \textit{cross-entropy loss}. \textit{Regularization loss} is only the function of weights, and its goal is to penalize large weights.  

During backpropagation, \textit{optimizer} is used to update the weights of the network to minimize the loss function. The optimizer uses the gradient of the loss function to determine the direction toward the local minima. The size of the step we take in that direction is defined by parameter \textit{learning rate}.  Weights are updated after each pass of the batch through the network - one iteration. When all training data has passed through the network through batches, one \textit{epoch} has passed.

After each epoch, \textit{validation data} is used to determine the performance of our network. Validation loss is computed but itâ€˜s not used to update weights during backpropagation. During the training process, validation loss is most commonly used to adjust the learning rate through \textit{scheduler}. After the network is trained, the performance on the validation data is key to adjusting the hyperparameters of the network to achieve better results. 

The network is trained for a certain number of epochs, which is a hyperparameter defined by the user. However, to avoid overfitting of data, \textit{early stopping algorithm} is deployed, which stops the process of training if needed.  

\input{software design/network/activation_function}
\input{software design/network/loss_function}
\input{software design/network/optimizer}
\input{software design/network/scheduler}
\input{software design/network/early_stopping}
\input{software design/network/dropout}
\input{software design/network/augumentation}







%%%%%%%%%%%%%%%%%%%%%%% alternatives %%%%%%%%%%%%%%
%However to find the minima, we need to move in the negative direction of the gradient.  While the gradient represents the steepest slope 

%The gradient of the loss function is computed over the whole batch of images. Using the optimizer, 

% there are several types of optimizer, and each have different parameters. 

%Learning rate determines the speed of the optimalization. 
%Optimizer uses the gradient of the loss function to determine the direction of steepest decrease to minimize the loss.

%the goal of the optimizer is to find a set of weights that minimizes the loss function. The gradient of the loss function is calculated to determine the direction of the steepest increase. 

% The network usually has a predefined number of epochs,

