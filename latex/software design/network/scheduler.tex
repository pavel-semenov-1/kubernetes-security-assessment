\subsubsection{Scheduler}

Neural networks require a large number of hyperparameters to tune. Some of them don't affect the performance to such an extent while others are essential for the model to be set properly. One of such essential ones is the learning rate of optimizers. If it's too low the training process is excessively long. On the other hand, if we set it to be too large, the optimization diverges from the minima. It may also not be optimal for the learning rate to stay the same during the whole training process. When we are near the local minima, large steps could cause us to stray away from it, while in the beginning, it could help us reach the minima faster.
One of the ways to improve the optimization process is to use learning rate schedulers. There are several different types of schedulers. Some reduce the learning rate after each epoch, others reduce it after they have reached a certain epoch or after a given set of predefined epochs. The reduction may be done by multiplying the learning rate by some factor or the user defines the specific set of learning rates that will be used. One of the common ways is to use the validation loss to determine when to reduce the learning rate.
