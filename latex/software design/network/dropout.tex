\subsubsection{Dropout}
Another way to prevent the network from overfitting is to use dropout. It is a technique that randomly removes some neurons from the computation of the network. Each neuron is dropped randomly with the probability of p, which is a hyperparameter defined by the user. This allows the network to not rely on specific neurons, prevents layers from co-adaptation, and makes the model essentially more robust. The dropout can be applied to fully-connected layers as well as convolutional layers, but not to the output layer as this would remove prediction for a certain class \cite{standford}.
